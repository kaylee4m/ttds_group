一部分indexing模块，储存了每个term的tf和df，可以用来计算某个query的tfidf或者BM2.5值

输入： 基于小田count_data.py里的读取方法，读取metadata.json.gz文件

读取以后将每一个文章的title与abstract作为一个整体进行处理（可能会给title里出现的term更多的权重）


预处理：用nltk的分词（nltk.word_tokenize），去除stopwords
        不用stemming，用steamming可能会影响一些术语和专业词汇的：text classification>text classi,
	而且也会影响我们用

储存tf和df的结构：posting_lists=｛term:[df,[{doc_id:tf}]]｝

例如 query里包含convolutional,
那么postings_lists['convolutional']=[3, [{'0704.0282': 2}, {'0704.0361': 1}, {'0704.1411': 2}]]
意思是说convolutional这个词在三个文件里出现了（df）,而且这个词在这三个文件中出现的次数分别是2,1,2

输出：用sqlite3将上述结构导出到数据库

更新：
加了stemming.
改了一下结构，这次可以显示某个单词出现的位置，tf和df也可以用len()来算，生成速度比之前那种结构慢一些
postings_lists[''lamin''] = [{'0704.2152': [41]},
 		       {'0706.0677': [0, 12, 16, 27]},
		       {'0706.1313': [5, 9, 17, 26]}]

关于其他限定条件(文章类型，作者)，可以讨论以后再建其他格式的索引，有id什么都好说。

存储：想直接用json提供的接口：

js = json.dumps(postings_lists)
file = open('indexdict.txt', 'w')
file.write(js)
file.close()

读取也用json读取：

with open('indexdict.txt','r') as dictfile:
    js = dictfile.read()
    index_dic = json.loads(js)

index_dic[''lamin''] = [{'0704.2152': [41]},
 		 {'0706.0677': [0, 12, 16, 27]},
		 {'0706.1313': [5, 9, 17, 26]}]


